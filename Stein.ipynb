{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stein.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-c6hPMDLqceL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Kernelized Stein Gradient\n",
        "\n",
        "> Notebook accompanying the original post [here](https://www.sanyamkapoor.com/machine-learning/stein-gradient).\n",
        "\n",
        "## Install Dependencies\n",
        "\n",
        "We will use PyTorch for all our differentiation needs and `plotly` for plotting."
      ]
    },
    {
      "metadata": {
        "id": "bBgcRDcqqb3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip install plotly>=3.6.0 numpy>=1.16 torch>=1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q6pVRbE3qVGH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "from plotly.colors import DEFAULT_PLOTLY_COLORS\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3m7RgEmpwsgx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Drawing Utilities"
      ]
    },
    {
      "metadata": {
        "id": "DXJT7udKwvHc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def configure_plotly_browser_state():\n",
        "  \"\"\"\n",
        "  @NOTE: Run this in each cell before plotting in Google Colab\n",
        "  \"\"\"\n",
        "  \n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n",
        "  py.init_notebook_mode(connected=True)\n",
        "\n",
        "  \n",
        "def get_pos_xy(d=10.0, step=0.1):\n",
        "    xv, yv = torch.meshgrid([\n",
        "        torch.arange(-d, d, step), \n",
        "        torch.arange(-d, d, step)\n",
        "    ])\n",
        "    pos_xy = torch.cat((xv.unsqueeze(-1), yv.unsqueeze(-1)), dim=-1)\n",
        "    return xv, yv, pos_xy\n",
        "\n",
        "\n",
        "def get_scatter_trace(X, color, name='', mode='markers'):\n",
        "    return go.Scatter(\n",
        "        name=name,\n",
        "        x=X[:, 0],\n",
        "        y=X[:, 1],\n",
        "        mode=mode,\n",
        "        line=dict(\n",
        "            width=1,\n",
        "            color='rgba(255,0,0,0.5)',\n",
        "        ),\n",
        "        marker=dict(\n",
        "            size=5,\n",
        "            color=color,\n",
        "            showscale=False,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_particles(P, X, name):\n",
        "    xv, yv, pos_xy = get_pos_xy()\n",
        "    orig_trace = go.Heatmap(\n",
        "        name=name,\n",
        "        x=xv.numpy().flatten(),\n",
        "        y=yv.numpy().flatten(),\n",
        "        z=P.log_prob(pos_xy.to(device)).exp().cpu().numpy().flatten(),\n",
        "        colorscale='Viridis',\n",
        "        showscale=False,\n",
        "    )\n",
        "    \n",
        "    layout = go.Layout(\n",
        "        autosize=False,\n",
        "        width=1000,\n",
        "        height=700\n",
        "    )\n",
        "\n",
        "    fig = go.Figure(layout=layout,\n",
        "                    data=[orig_trace, get_scatter_trace(X.cpu().numpy(), 'red', name='Stein Particles')])\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KO99dFrqsbVP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RBF Kernel\n",
        "\n",
        "In these experiments, we will use the *rbf* kernel. The kernel is defined as the squared exponential distance between the two vectors, parametrized by a bandwidth argument $\\sigma$.\n",
        "\n",
        "$$\n",
        "k_{rbf}(\\mathbf{x}, \\mathbf{x}^\\prime) = \\exp{-\\frac{1}{2\\sigma^2}||\\mathbf{x}-\\mathbf{x}^\\prime||^2}\n",
        "$$\n",
        "\n",
        "A vectorized version of the  kernel is given below. A few notes on the implementation follow."
      ]
    },
    {
      "metadata": {
        "id": "aBqMhW0JrX4L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RBF(torch.nn.Module):\n",
        "  def __init__(self, sigma=None):\n",
        "    super(RBF, self).__init__()\n",
        "\n",
        "    self.sigma = sigma\n",
        "\n",
        "  def forward(self, X, Y):\n",
        "    XX = X.matmul(X.t())\n",
        "    XY = X.matmul(Y.t())\n",
        "    YY = Y.matmul(Y.t())\n",
        "\n",
        "    dnorm2 = -2 * XY + XX.diag().unsqueeze(1) + YY.diag().unsqueeze(0)\n",
        "\n",
        "    # Apply the median heuristic (PyTorch does not give true median)\n",
        "    if self.sigma is None:\n",
        "      np_dnorm2 = dnorm2.detach().cpu().numpy()\n",
        "      h = np.median(np_dnorm2) / (2 * np.log(X.size(0) + 1))\n",
        "      sigma = np.sqrt(h).item()\n",
        "    else:\n",
        "      sigma = self.sigma\n",
        "\n",
        "    gamma = 1.0 / (1e-8 + 2 * sigma ** 2)\n",
        "    K_XY = (-gamma * dnorm2).exp()\n",
        "\n",
        "    return K_XY\n",
        "  \n",
        "# Let us initialize a reusable instance right away.\n",
        "K = RBF()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cJMG52Xotnt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Selecting the bandwidth parameter $\\sigma$ may be a painful task in itself. A popular heuristic chosen in literature is the *median* heuristic where we choose the bandwidth to be\n",
        "\n",
        "$$\n",
        "\\sigma^2 = \\frac{median^2}{2 \\log{n}}\n",
        "$$\n",
        "\n",
        "the median among distance of all pairs. This allows for gradient contribution from all the pairs when computing the gradient of the kernel during simulation of the ODE. Note that we use the `numpy` median function because the PyTorch median function does not behave as expected when the number of elements are even (and does not return the mean of the two central elements).\n",
        "\n",
        "## Stein Variational Gradient Descent\n",
        "\n",
        "We now simulate the following ODE for each particle $x_j$ in the system.\n",
        "\n",
        "$$\n",
        "\\dot{x}_j = \\frac{1}{n} \\sum_{j = 1}^n \\left[ k(x_j, x) \\nabla_{x_j} \\log{p(x_j)} + \\nabla_{x_j} k(x_j, x)  \\right]\n",
        "$$\n",
        "\n",
        "For stability reasons, we use Adagrad to allow for adaptive step size during the simulation. In fact, Adagrad can be replaced by any of the adaptive step size or gradient techniques from Gradient Descent like Adam. For our puposes, Adagrad works just well enough as we will see in the results below.\n",
        "\n",
        "This is encapsulated in the `step` function below."
      ]
    },
    {
      "metadata": {
        "id": "2fPfeNqDrYgo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SVGD:\n",
        "  def __init__(self, P, K, eta=1e-2, rho=0.9):\n",
        "    self.P = P\n",
        "    self.K = K\n",
        "    self.eta = eta\n",
        "    self.rho = rho\n",
        "\n",
        "    self._phi_est = None\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self._phi_est = None\n",
        "\n",
        "  def step(self, X):\n",
        "    X = X.detach().requires_grad_(True)\n",
        "\n",
        "    log_prob = self.P.log_prob(X)\n",
        "    score_func = torch.autograd.grad(log_prob, X,\n",
        "                                     grad_outputs=torch.ones_like(log_prob),\n",
        "                                     only_inputs=True)[0]\n",
        "\n",
        "    K_XX = self.K(X, X.detach())\n",
        "    grad_K = -torch.autograd.grad(K_XX, X,\n",
        "                                  grad_outputs=torch.ones_like(K_XX),\n",
        "                                  only_inputs=True)[0]\n",
        "\n",
        "    phi = (K_XX.detach().matmul(score_func) + grad_K) / X.size(0)\n",
        "\n",
        "    if self._phi_est is None:\n",
        "      self._phi_est = phi ** 2\n",
        "    else:\n",
        "      self._phi_est = self.rho * self._phi_est + (1 - self.rho) * phi ** 2\n",
        "\n",
        "    grad = phi / (1e-8 + self._phi_est.sqrt())\n",
        "\n",
        "    return X.detach() + self.eta * grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vt3xvYpXwSVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "\n",
        "## Unimodal Gaussian\n",
        "\n",
        "We will first run this on a Unimodal Gaussian. We initialize the particles in an overdispersed manner and see how they converge around the typical set of the distribution.\n",
        "\n",
        "**NOTE**: Try increasing the number of particles $n$ and different initializations to see how the particles distribute themselves."
      ]
    },
    {
      "metadata": {
        "id": "uOtUzHZ3r6Ym",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gauss = torch.distributions.MultivariateNormal(torch.Tensor([-0.6871,0.8010]).to(device),\n",
        "        covariance_matrix=5 * torch.Tensor([[0.2260,0.1652],[0.1652,0.6779]]).to(device))\n",
        "\n",
        "svgd = SVGD(gauss, K, rho=0.9, eta=1e-2)\n",
        "\n",
        "n = 100\n",
        "X = (5 * torch.randn(n, *gauss.event_shape)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pggqMNETzJsm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us see how this overdispersed initialization looks like. Note that initializations much farther away from the typical set of the distributions may take longer to converge."
      ]
    },
    {
      "metadata": {
        "id": "O08fIdmvw8tG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "configure_plotly_browser_state()\n",
        "fig = plot_particles(gauss, X, 'Normal')\n",
        "py.iplot(fig, config=dict(showLink=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PiQGIBYHyQzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for _ in range(1000):\n",
        "    X = svgd.step(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aQmwFZxXyjIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "configure_plotly_browser_state()\n",
        "fig = plot_particles(gauss, X, 'Normal')\n",
        "py.iplot(fig, config=dict(showLink=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRbKFSzL0Eoh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mixture of Gaussians\n",
        "\n",
        "The exact same simulation without any manual fine tuning works even for a multimodal Gaussian. We will first create a generic PyTorch distribution which can help us build multiple kinds of Mixture of Gaussians."
      ]
    },
    {
      "metadata": {
        "id": "VE8ANKLgy1PH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MoG(torch.distributions.Distribution):\n",
        "  def __init__(self, loc, covariance_matrix):\n",
        "    self.num_components = loc.size(0)\n",
        "    self.loc = loc\n",
        "    self.covariance_matrix = covariance_matrix\n",
        "\n",
        "    self.dists = [\n",
        "      torch.distributions.MultivariateNormal(mu, covariance_matrix=sigma)\n",
        "      for mu, sigma in zip(loc, covariance_matrix)\n",
        "    ]\n",
        "    \n",
        "    super(MoG, self).__init__(torch.Size([]), torch.Size([loc.size(-1)]))\n",
        "\n",
        "  @property\n",
        "  def arg_constraints(self):\n",
        "    return self.dists[0].arg_constraints\n",
        "\n",
        "  @property\n",
        "  def support(self):\n",
        "    return self.dists[0].support\n",
        "\n",
        "  @property\n",
        "  def has_rsample(self):\n",
        "    return False\n",
        "\n",
        "  def log_prob(self, value):\n",
        "    return torch.cat(\n",
        "      [p.log_prob(value).unsqueeze(-1) for p in self.dists], dim=-1).logsumexp(dim=-1)\n",
        "\n",
        "  def enumerate_support(self):\n",
        "    return self.dists[0].enumerate_support()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q7QMpkbd05Wl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mixture of Two Gaussians\n",
        "\n",
        "Here we create a mixture of two Gaussians where the means are symmetrically placed at $x=|5|$ and the covariance matrix is given by $\\begin{pmatrix}0.5 & 0.5 \\\\ 0.5 & 0.5\\end{pmatrix}$."
      ]
    },
    {
      "metadata": {
        "id": "jXFYSu2P0vNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MoG2(MoG):\n",
        "  def __init__(self, device=None):\n",
        "    loc = torch.Tensor([[-5.0, 0.0], [5.0, 0.0]]).to(device)\n",
        "    cov = torch.Tensor([0.5, 0.5]).diag().unsqueeze(0).repeat(2, 1, 1).to(device)\n",
        "\n",
        "    super(MoG2, self).__init__(loc, cov)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VN47ygkV1tyL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mog2 = MoG2(device=device)\n",
        "\n",
        "svgd = SVGD(mog2, K, rho=0.9, eta=1e-2)\n",
        "\n",
        "n = 100\n",
        "X = (5 * torch.randn(n, *mog2.event_shape)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fea03JNU18h3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "configure_plotly_browser_state()\n",
        "fig = plot_particles(mog2, X, 'Mixture of Two Gaussians')\n",
        "py.iplot(fig, config=dict(showLink=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGVeprix13St",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for _ in range(1000):\n",
        "    X = svgd.step(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bK-t1DjA14Hq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "configure_plotly_browser_state()\n",
        "fig = plot_particles(mog2, X, 'Mixture of Two Gaussians')\n",
        "py.iplot(fig, config=dict(showLink=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VrxHTrcO1VJT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mixture of Six Gaussians\n",
        "\n",
        "Here we create a mixture of six Gaussians where the means are spread around a circle of radius $5$and the covariance matrix is given by $\\begin{pmatrix}0.5 & 0.5 \\\\ 0.5 & 0.5\\end{pmatrix}$."
      ]
    },
    {
      "metadata": {
        "id": "0HtejqVu1m3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MoG6(MoG):\n",
        "  def __init__(self, device=None):\n",
        "    def _compute_mu(i):\n",
        "      return 5.0 * torch.Tensor([[\n",
        "        torch.tensor(i * math.pi / 3.0).sin(),\n",
        "        torch.tensor(i * math.pi / 3.0).cos()]])\n",
        "\n",
        "    loc = torch.cat([_compute_mu(i) for i in range(1, 7)], dim=0).to(device)\n",
        "    cov = torch.Tensor([0.5, 0.5]).diag().unsqueeze(0).to(device).repeat(6, 1, 1)\n",
        "\n",
        "    super(MoG6, self).__init__(loc, cov)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hm0tE42-2Yjw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mog6 = MoG6(device=device)\n",
        "\n",
        "svgd = SVGD(mog6, K, rho=0.9, eta=1e-2)\n",
        "\n",
        "n = 100\n",
        "X = (5 * torch.randn(n, *mog6.event_shape)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-63mcO8T2cyI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "configure_plotly_browser_state()\n",
        "fig = plot_particles(mog6, X, 'Mixture of Six Gaussians')\n",
        "py.iplot(fig, config=dict(showLink=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vG_nTGbx2gBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for _ in range(1000):\n",
        "    X = svgd.step(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6IrYkdh2g8J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "configure_plotly_browser_state()\n",
        "fig = plot_particles(mog6, X, 'Mixture of Six Gaussians')\n",
        "py.iplot(fig, config=dict(showLink=False))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}